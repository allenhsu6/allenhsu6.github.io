<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Xu'Log</title><link>https://allenhsu6.github.io/posts/</link><description>Recent content in Posts on Xu'Log</description><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Mon, 01 Dec 2025 19:50:00 +0800</lastBuildDate><atom:link href="https://allenhsu6.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>国债期货：个人量化交易者的破局之道</title><link>https://allenhsu6.github.io/posts/single_people_should_focused/</link><pubDate>Mon, 01 Dec 2025 19:50:00 +0800</pubDate><guid>https://allenhsu6.github.io/posts/single_people_should_focused/</guid><description>在机构林立的国债期货市场，个人交易者如何利用&amp;#39;船小好掉头&amp;#39;的优势，寻找机构非盈利导向交易留下的利润空间？</description></item><item><title>ATR波动率突破策略：自适应止损的数学原理</title><link>https://allenhsu6.github.io/posts/strategy-analysis-01-atr-breakout/</link><pubDate>Sun, 16 Nov 2025 10:00:00 +0800</pubDate><guid>https://allenhsu6.github.io/posts/strategy-analysis-01-atr-breakout/</guid><description>ATR指标的数学推导、波动率突破策略的完整实现、回测验证与实盘部署要点</description></item><item><title>基于vnpy的量化交易系统架构与策略分类</title><link>https://allenhsu6.github.io/posts/quant-system-overview/</link><pubDate>Sat, 15 Nov 2025 23:45:00 +0800</pubDate><guid>https://allenhsu6.github.io/posts/quant-system-overview/</guid><description>vnpy 4.1.0量化交易系统的完整架构设计、策略分类体系，以及国债期货和股指期货的交易策略全景图</description></item><item><title>Hugo + PaperMod 博客写作完全指南</title><link>https://allenhsu6.github.io/posts/hugo-writing-guide/</link><pubDate>Sat, 15 Nov 2025 14:30:00 +0800</pubDate><guid>https://allenhsu6.github.io/posts/hugo-writing-guide/</guid><description>全面介绍如何在 Hugo + PaperMod 主题博客中编写文章，包括 Front Matter 配置、Markdown 语法、主题特有功能、数学公式和最佳实践</description></item><item><title>Deepseek论文解读：MLA</title><link>https://allenhsu6.github.io/posts/deepseek_series1_mla/</link><pubDate>Sat, 15 Feb 2025 10:41:35 +0800</pubDate><guid>https://allenhsu6.github.io/posts/deepseek_series1_mla/</guid><description>&lt;p&gt;Multihead Latent Attention (MLA)是Deepseek提出的一种新的attention机制，用于减少推理时的内存瓶颈，同时兼顾性能提升和缓存效率。本文将对MLA的原理、实现和效果进行解读。&lt;/p&gt;
&lt;h1 id="notations"&gt;Notations&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symbol&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$d_h$&lt;/td&gt;
&lt;td&gt;dimension of embedding per attention head&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$d_c$&lt;/td&gt;
&lt;td&gt;the KV compression dimension in MLA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$d_r^R$&lt;/td&gt;
&lt;td&gt;the per-head dimension of the decoupled queries and key in MLA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$n_h$&lt;/td&gt;
&lt;td&gt;number of attention heads&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$l$&lt;/td&gt;
&lt;td&gt;the transformer layer number&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$h_t \in \mathbb{R}^{d}$&lt;/td&gt;
&lt;td&gt;the attention input of $t$-th token at an attention layer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$u_t \in \mathbb{R}^{d}$&lt;/td&gt;
&lt;td&gt;the output hidden of $t$-th token at an attention layer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="背景"&gt;背景&lt;/h1&gt;
&lt;p&gt;auto-regressive的LLM主要是decoder的架构，基于先前生成的tokens来预测下一个token。生成过程是顺序的，会用到历史token的KV来保证生成的连贯性，直到最大长度或者生成结束符。 每一步的output作为下次input时，需要进行tokenlizer、embedding、MLP投影生成Q，K，V，这时为了避免重复计算，会将KV做cache；但是在常用MHA中，kVcache的大小会随着token长度的增加而增加，导致内存瓶颈。&lt;/p&gt;</description></item></channel></rss>